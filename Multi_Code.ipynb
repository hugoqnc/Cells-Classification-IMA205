{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hugo Queinnec - IMA205**\n",
    "# Pap smear cells multi-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports et chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "\n",
    "# for reading and displaying images\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# methods\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#features\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute matthews_correlation_coefficient\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Group 0: 682 occurences\nGroup 1: 551 occurences\nGroup 2: 69 occurences\nGroup 3: 105 occurences\nGroup 4: 127 occurences\nGroup 5: 102 occurences\nGroup 6: 138 occurences\nGroup 7: 578 occurences\nGroup 8: 569 occurences\n"
     ]
    }
   ],
   "source": [
    "# Load Train Data\n",
    "\n",
    "#____ CHANGE WORKING DIRECTORY HERE________________________\n",
    "Working_directory=\"./\"\n",
    "#__________________________________________________________\n",
    "\n",
    "df = pd.read_csv(Working_directory+'metadataTrain.csv') # reading data\n",
    "train_y = df['GROUP'].values # 1 for Melanoma and 0 for healthy\n",
    "class_names = [\"normal\",\"abnormal\"]\n",
    "N=train_y.shape[0]\n",
    "\n",
    "occurences = np.bincount(np.array(train_y))\n",
    "for i in range(occurences.shape[0]):\n",
    "    print(\"Group \"+str(i)+\": \"+str(occurences[i])+\" occurences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Déterminer des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading training images\n",
    "X_labels = df['ID'].values\n",
    "train_img = [0]*N\n",
    "train_imgSegCyt = [0]*N\n",
    "train_imgSegNuc = [0]*N\n",
    "\n",
    "i = 0\n",
    "for id in X_labels:\n",
    "    # defining the image path\n",
    "    image_path = Working_directory+'Train/Train/' + str(id) + '.bmp'\n",
    "    image_pathSegCyt = Working_directory+'Train/Train/' + str(id) + '_segCyt.bmp'\n",
    "    image_pathSegNuc = Working_directory+'Train/Train/' + str(id) + '_segNuc.bmp'\n",
    "\n",
    "    img = imread(image_path)\n",
    "    train_img[i]=img\n",
    "\n",
    "    imgSegCyt = imread(image_pathSegCyt)\n",
    "    train_imgSegCyt[i]=imgSegCyt\n",
    "\n",
    "    imgSegNuc = imread(image_pathSegNuc)\n",
    "    train_imgSegNuc[i]=imgSegNuc\n",
    "\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeManualFeaturesNormalized(img, maskCyt, maskNuc, features, colorFeatures, featuresToNormalize, tupleFeatures):\n",
    "    errorCount = 0\n",
    "\n",
    "    numberOfColorDescriptors = 6\n",
    "    numberOfFeatures = 2*len(features) + numberOfColorDescriptors*len(colorFeatures) + len(featuresToNormalize) + 4*len(tupleFeatures)\n",
    "    \n",
    "\n",
    "    train_img_features = np.zeros((1, numberOfFeatures))\n",
    "\n",
    "    labelsCyt = measure.label(np.round(maskCyt), background=0)\n",
    "    regionsCytR = measure.regionprops(labelsCyt, img[:,:,0])\n",
    "    regionsCytG = measure.regionprops(labelsCyt, img[:,:,1])\n",
    "    regionsCytB = measure.regionprops(labelsCyt, img[:,:,2])\n",
    "\n",
    "    labelsNuc = measure.label(np.round(maskNuc), background=0)\n",
    "    regionsNucR = measure.regionprops(labelsNuc, img[:,:,0])\n",
    "    regionsNucG = measure.regionprops(labelsNuc, img[:,:,1])\n",
    "    regionsNucB = measure.regionprops(labelsNuc, img[:,:,2])\n",
    "\n",
    "    # maskExt = np.int64(255*np.ones((len(maskCyt), len(maskCyt[0]))) - maskCyt - maskNuc)\n",
    "    # labelsExt = measure.label(np.round(maskExt), background=0)\n",
    "    # regionsExtR = measure.regionprops(labelsExt, img[:,:,0])\n",
    "    # regionsExtG = measure.regionprops(labelsExt, img[:,:,1])\n",
    "    # regionsExtB = measure.regionprops(labelsExt, img[:,:,2])\n",
    "\n",
    "    for j in range(len(colorFeatures)):\n",
    "        feature = colorFeatures[j]\n",
    "        if(len(regionsCytR)!=0):\n",
    "            train_img_features[0,j*3] = getattr(regionsCytR[0], feature)\n",
    "            train_img_features[0,j*3+1] = getattr(regionsCytG[0], feature)\n",
    "            train_img_features[0,j*3+2] = getattr(regionsCytB[0], feature)\n",
    "        else:\n",
    "            train_img_features[0,j*3] = None\n",
    "            train_img_features[0,j*3+1] = None\n",
    "            train_img_features[0,j*3+2] = None\n",
    "            errorCount+=1\n",
    "\n",
    "    for j in range(len(colorFeatures)):\n",
    "        feature = colorFeatures[j]\n",
    "        if(len(regionsNucR)!=0):\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)] = getattr(regionsNucR[0], feature)\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)+1] = getattr(regionsNucG[0], feature)\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)+2] = getattr(regionsNucB[0], feature)\n",
    "        else:\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)] = None\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)+1] = None\n",
    "            train_img_features[0,j*3+3*len(colorFeatures)+2] = None\n",
    "            errorCount+=1\n",
    "\n",
    "    # for j in range(len(colorFeatures)):\n",
    "    #     feature = colorFeatures[j]\n",
    "    #     if(len(regionsExtR)!=0):\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)+1] = getattr(regionsExtG[0], feature)\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)+2] = getattr(regionsExtB[0], feature)\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)] = getattr(regionsExtR[0], feature)\n",
    "    #     else:\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)] = None\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)+1] = None\n",
    "    #         train_img_features[0,j*3+6*len(colorFeatures)+2] = None\n",
    "    #         errorCount+=1\n",
    "    \n",
    "    for j in range(len(featuresToNormalize)):\n",
    "        feature = featuresToNormalize[j]\n",
    "        if(len(regionsCytR)!=0 and len(regionsNucR)!=0 and getattr(regionsCytR[0], feature)!=0):\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)] = getattr(regionsNucR[0], feature)/getattr(regionsCytR[0], feature)\n",
    "        else:\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)] = None\n",
    "            #errorCount+=1\n",
    "\n",
    "\n",
    "    for j in range(len(tupleFeatures)):\n",
    "        feature = tupleFeatures[j]\n",
    "        if(len(regionsCytR)!=0 and len(regionsNucR)!=0):\n",
    "            x1,y1 = getattr(regionsCytR[0], feature)\n",
    "            x2,y2 = getattr(regionsNucR[0], feature)\n",
    "            train_img_features[0,j*4+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = x1/y1\n",
    "            train_img_features[0,j*4+1+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = x2/y2\n",
    "            train_img_features[0,j*4+2+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = x1/x2\n",
    "            train_img_features[0,j*4+3+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = y1/y2\n",
    "        else:\n",
    "            train_img_features[0,j*4+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = None\n",
    "            train_img_features[0,j*4+1+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = None\n",
    "            train_img_features[0,j*4+2+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = None\n",
    "            train_img_features[0,j*4+3+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)] = None\n",
    "            #errorCount+=1\n",
    "    \n",
    "    for j in range(len(features)):\n",
    "        feature = features[j]\n",
    "        if feature=='symmetry_lr':\n",
    "            diff_area_h_cyt = np.count_nonzero(maskCyt * ~np.fliplr(maskCyt))\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)] = diff_area_h_cyt/np.count_nonzero(maskCyt)\n",
    "        elif feature=='symmetry_ud':\n",
    "            diff_area_v_cyt = np.count_nonzero(maskCyt * ~np.flipud(maskCyt))\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)] = diff_area_v_cyt/np.count_nonzero(maskCyt)\n",
    "        elif feature=='perimeter_norm':\n",
    "            a = getattr(regionsCytR[0], 'minor_axis_length')\n",
    "            p = getattr(regionsCytR[0], 'perimeter')\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)] = p/a\n",
    "        else:\n",
    "            if(len(regionsCytR)!=0):\n",
    "                train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)] = getattr(regionsCytR[0], feature)\n",
    "            else:\n",
    "                train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)] = None\n",
    "                #errorCount+=1\n",
    "\n",
    "    for j in range(len(features)):\n",
    "        feature = features[j]\n",
    "        if(len(regionsNucR)!=0):\n",
    "            if feature=='symmetry_lr':\n",
    "                diff_area_h_nuc = np.count_nonzero(maskNuc * ~np.fliplr(maskNuc))\n",
    "                train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)+len(features)] = diff_area_h_nuc/np.count_nonzero(maskNuc)\n",
    "            elif feature=='symmetry_ud':\n",
    "                diff_area_v_cyt = np.count_nonzero(maskCyt * ~np.flipud(maskCyt))\n",
    "                train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)+len(features)] = diff_area_v_cyt/np.count_nonzero(maskCyt)\n",
    "            elif feature=='perimeter_norm':\n",
    "                a = getattr(regionsNucR[0], 'minor_axis_length')\n",
    "                p = getattr(regionsNucR[0], 'perimeter')\n",
    "                train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)+len(features)] = p/a\n",
    "            else:\n",
    "               train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)+len(features)] = getattr(regionsNucR[0], feature)\n",
    "        else:\n",
    "            train_img_features[0,j+numberOfColorDescriptors*len(colorFeatures)+len(featuresToNormalize)+4*len(tupleFeatures)+len(features)] = None\n",
    "            #errorCount+=1\n",
    "\n",
    "    \n",
    "\n",
    "    return train_img_features, errorCount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLinearRegression(train_img_features,train_y):\n",
    "    resolution_param = 150  \n",
    "    regr = LinearRegression()\n",
    "    regr.fit(train_img_features, train_y)\n",
    "    return regr\n",
    "\n",
    "def fitLDA(train_img_features,train_y):\n",
    "    resolution_param = 150  \n",
    "    clf_LDA = LinearDiscriminantAnalysis()\n",
    "    clf_LDA.fit(train_img_features, train_y)\n",
    "    return clf_LDA\n",
    "\n",
    "def fitQDA(train_img_features,train_y):\n",
    "    resolution_param = 150  \n",
    "    clf_QDA = QuadraticDiscriminantAnalysis()\n",
    "    clf_QDA.fit(train_img_features, train_y)\n",
    "    return clf_QDA\n",
    "\n",
    "def fitBayes(train_img_features,train_y):\n",
    "    resolution_param = 150  \n",
    "    clf_GNB = GaussianNB()\n",
    "    clf_GNB.fit(train_img_features, train_y)\n",
    "    return clf_GNB\n",
    "\n",
    "def fitKNN(train_img_features,train_y):\n",
    "    resolution_param = 150  \n",
    "    clf_KNN = KNeighborsClassifier()\n",
    "    clf_KNN.n_neighbors=5\n",
    "    clf_KNN.fit(train_img_features, train_y)\n",
    "    return clf_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de prédiction adaptée au splits du training set\n",
    "\n",
    "def predictForTestSplit(one_split_test_img, one_split_test_imgCyt, one_split_test_imgNuc, classifier):\n",
    "    #compute features\n",
    "    f = computeManualFeaturesNormalized(one_split_test_img, one_split_test_imgCyt, one_split_test_imgNuc, features, colorFeatures, featuresToNormalize, tupleFeatures)[0]\n",
    "\n",
    "    for i in range(len(f[0])): #cleaning None values\n",
    "        if np.isnan(f[0][i]):\n",
    "            f[0][i] = meanOfTrainingFeatures[i]\n",
    "\n",
    "    f = scaler.transform(f) #scale\n",
    "\n",
    "    y_test = classifier.predict(f)\n",
    "\n",
    "    return y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEntireTestSplit(split_test_img, split_test_imgSegCyt, split_test_imgSegNuc, classifier):\n",
    "    allFeatures = np.zeros((len(split_test_img), numberOfFeatures))\n",
    "\n",
    "    for i in range(len(split_test_img)):\n",
    "        f = computeManualFeaturesNormalized(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i], features, colorFeatures, featuresToNormalize, tupleFeatures)[0]\n",
    "        \n",
    "        for j in range(len(f[0])): #cleaning None values\n",
    "            if np.isnan(f[0][j]):\n",
    "                f[0][j] = meanOfTrainingFeatures[j]\n",
    "\n",
    "        f = scaler.transform(f) #scale\n",
    "\n",
    "        allFeatures[i,:] = f[0]\n",
    "\n",
    "    \n",
    "    if boolDimensionReduction:\n",
    "        allFeatures = pca.transform(allFeatures)\n",
    "    \n",
    "    y_test = classifier.predict(allFeatures)\n",
    "\n",
    "    return y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_int_round(z, n_class): #for Linear Regression\n",
    "    # rounding needed to go from real to integer values \n",
    "    output = np.round(z).astype(int)\n",
    "    if isinstance(z, np.ndarray):\n",
    "        j = z < 0\n",
    "        output[j] = 0\n",
    "        k = z > n_class - 1\n",
    "        output[k] = n_class - 1\n",
    "    else:\n",
    "        if output < 0:\n",
    "            output = 0\n",
    "        else:\n",
    "            if output > n_class - 1:\n",
    "                output = n_class - 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission csv\n",
    "\n",
    "def submissionCSV(fileName, classifier):\n",
    "\n",
    "    sample = pd.read_csv(Working_directory+'SampleSubmission.csv') # reading data\n",
    "    X_test_values = sample['ID'].values\n",
    "    N = X_test_values.shape[0]\n",
    "\n",
    "    test_img = [0]*N\n",
    "    test_imgSegCyt = [0]*N\n",
    "    test_imgSegNuc = [0]*N\n",
    "\n",
    "    i = 0\n",
    "    for id in X_test_values:\n",
    "        # defining the image path\n",
    "        image_path = Working_directory+'Test/Test/' + str(id) + '.bmp'\n",
    "        image_pathSegCyt = Working_directory+'Test/Test/' + str(id) + '_segCyt.bmp'\n",
    "        image_pathSegNuc = Working_directory+'Test/Test/' + str(id) + '_segNuc.bmp'\n",
    "\n",
    "        img = imread(image_path)\n",
    "        test_img[i]=img\n",
    "\n",
    "        imgSegCyt = imread(image_pathSegCyt)\n",
    "        test_imgSegCyt[i]=imgSegCyt\n",
    "\n",
    "        imgSegNuc = imread(image_pathSegNuc)\n",
    "        test_imgSegNuc[i]=imgSegNuc\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    two_columns = np.zeros((X_test_values.shape[0],2), dtype=int)\n",
    "    two_columns[:,0] = X_test_values\n",
    "\n",
    "    y_prediction = predictEntireTestSplit(test_img, test_imgSegCyt, test_imgSegNuc, classifier)\n",
    "    \n",
    "    for i in range(X_test_values.shape[0]):\n",
    "        two_columns[i,1] = y_prediction[i]\n",
    "\n",
    "    computedValues = pd.DataFrame(two_columns, columns=['ID','GROUP'])\n",
    "    computedValues.to_csv(Working_directory+fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tests\n",
    "On tranforme le train initial en deux ensembles train et test, pour obtenir plus facilement des scores, sans devoir passer par Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____to modify______________________\n",
    "boolSplitAndTestLocally = True #True : the following cells will print Test and Train accuracy | False : the followinf cells will print Train accuracy, and the last cell will export a CSV with the Train predictions of a chosen estimator\n",
    "boolDimensionReduction = False #compute a PCA before Non linear SVM, Boosting and MLP\n",
    "#___________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Group 0: 511 occurences\nGroup 1: 413 occurences\nGroup 2: 52 occurences\nGroup 3: 79 occurences\nGroup 4: 95 occurences\nGroup 5: 77 occurences\nGroup 6: 103 occurences\nGroup 7: 433 occurences\nGroup 8: 427 occurences\n\nGroup 0: 171 occurences\nGroup 1: 138 occurences\nGroup 2: 17 occurences\nGroup 3: 26 occurences\nGroup 4: 32 occurences\nGroup 5: 25 occurences\nGroup 6: 35 occurences\nGroup 7: 145 occurences\nGroup 8: 142 occurences\n"
     ]
    }
   ],
   "source": [
    "# Partage de l'ensemble de test initial\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    split_train_img, split_test_img, split_train_imgSegCyt, split_test_imgSegCyt, split_train_imgSegNuc, split_test_imgSegNuc, split_train_y, split_test_y = train_test_split(train_img, train_imgSegCyt, train_imgSegNuc, train_y, test_size=0.25, random_state=42, stratify=train_y)\n",
    "else:\n",
    "    split_train_img, split_train_imgSegCyt, split_train_imgSegNuc, split_train_y = train_img, train_imgSegCyt, train_imgSegNuc, train_y\n",
    "\n",
    "occurences = np.bincount(np.array(split_train_y))\n",
    "for i in range(occurences.shape[0]):\n",
    "    print(\"Group \"+str(i)+\": \"+str(occurences[i])+\" occurences\")\n",
    "\n",
    "print()\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    occurences = np.bincount(np.array(split_test_y))\n",
    "    for i in range(occurences.shape[0]):\n",
    "        print(\"Group \"+str(i)+\": \"+str(occurences[i])+\" occurences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul et pre-processing des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Errors in features: 39\n"
     ]
    }
   ],
   "source": [
    "split_N = split_train_y.shape[0]\n",
    "\n",
    "## LOT OF FEATURES\n",
    "# colorFeatures = ['mean_intensity', 'max_intensity', 'min_intensity']\n",
    "# featuresToNormalize = ['area', 'equivalent_diameter', 'perimeter', 'euler_number', 'convex_area', 'minor_axis_length', 'major_axis_length']\n",
    "# tupleFeatures = ['centroid']\n",
    "# features = ['solidity', 'eccentricity', 'extent','symmetry_lr', 'symmetry_ud']#, 'perimeter_norm']\n",
    "\n",
    "\n",
    "## FEW BEST FEATURES\n",
    "colorFeatures = ['mean_intensity', 'max_intensity', 'min_intensity']\n",
    "featuresToNormalize = ['area', 'equivalent_diameter', 'perimeter', 'euler_number']\n",
    "tupleFeatures = ['centroid']\n",
    "features = ['solidity']\n",
    "\n",
    "numberOfFeatures = 2*len(features) + 6*len(colorFeatures) + len(featuresToNormalize) + 4*len(tupleFeatures)\n",
    "train_img_features_0 = np.zeros((split_N, numberOfFeatures))\n",
    "\n",
    "errorCount = 0\n",
    "\n",
    "for i in range(split_N):\n",
    "    img = split_train_img[i]\n",
    "    maskCyt = split_train_imgSegCyt[i]\n",
    "    maskNuc = split_train_imgSegNuc[i]\n",
    "\n",
    "    f, e = computeManualFeaturesNormalized(img, maskCyt, maskNuc, features, colorFeatures, featuresToNormalize, tupleFeatures)\n",
    "    train_img_features_0[i] = f\n",
    "    errorCount+=e\n",
    "\n",
    "print(\"Errors in features: \" + str(errorCount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INITIALLY\n",
      "[139.83393065 125.73210201 149.50317725 187.47990868 177.83972603\n",
      " 196.42785388  84.57077626  74.23150685 102.22694064          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan          nan          nan          nan          nan\n",
      "          nan   0.76327158          nan]\n",
      "\n",
      "MEAN\n",
      "All features: (2190, 28) | Features without None: (223, 28)\n",
      "[141.29448314 133.10785472 155.68505198 187.98206278 178.367713\n",
      " 195.66367713  98.16143498  92.9955157  119.2690583   95.97494679\n",
      "  84.35509337 121.7404573  139.30044843 127.14349776 158.8161435\n",
      "  68.81165919  60.0941704   96.46188341   5.91407674   1.35662401\n",
      "   1.02648326   0.72892377   0.98219704   0.97826114   0.94641675\n",
      "   1.01868753   0.53386817   0.95341928]\n",
      "\n",
      "FINALLY\n",
      "Cleaned features: (2190, 28)\n",
      "[139.83393065 125.73210201 149.50317725 187.47990868 177.83972603\n",
      " 196.42785388  84.57077626  74.23150685 102.22694064  93.3690155\n",
      "  77.6886183  112.0341255  132.86799353 116.50724451 146.53178533\n",
      "  71.30070848  58.2375453   89.84977374   0.86692922   0.54356697\n",
      "   0.33756577   0.72892377   0.94614073   0.96841577   1.02239437\n",
      "   1.02686569   0.76327158   0.96942633]\n"
     ]
    }
   ],
   "source": [
    "def cleanNoneValues(train_img_features_0, verbose):\n",
    "    # replace missing values (None) of train_img_features\n",
    "    if verbose: print(\"INITIALLY\")\n",
    "    m0 = np.mean(train_img_features_0, axis=0)\n",
    "    if verbose: print(m0)\n",
    "\n",
    "    # compute mean of features\n",
    "    if verbose: print(\"\\nMEAN\")\n",
    "    featuresWithoutNone = train_img_features_0[~np.isnan(train_img_features_0).any(axis=1)]\n",
    "    if verbose: print(\"All features: \"+str(train_img_features_0.shape)+\" | Features without None: \"+str(featuresWithoutNone.shape))\n",
    "    m = np.mean(featuresWithoutNone, axis=0)\n",
    "    if verbose: print(m)\n",
    "\n",
    "    #replace missing values\n",
    "    if verbose: print(\"\\nFINALLY\")\n",
    "    train_img_features = np.array([[line[i] if ~np.isnan(line[i]) else m[i] for i in range(len(line))] for line in train_img_features_0])\n",
    "    if verbose: print(\"Cleaned features: \"+str(train_img_features.shape))\n",
    "    m1 = np.mean(train_img_features, axis=0)\n",
    "    if verbose: print(m1)\n",
    "\n",
    "    return train_img_features, m\n",
    "\n",
    "train_img_features, meanOfTrainingFeatures = cleanNoneValues(train_img_features_0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-4.55982284e-15 -1.20046033e-16 -1.34544836e-16 -2.15150069e-16\n -4.05053971e-17  1.01390231e-19  2.25694653e-16  1.26737788e-17\n  1.28410727e-16  2.93788332e-15 -7.47575517e-15  8.40436295e-16\n  1.27913915e-15  2.63310429e-16 -1.03534634e-15  1.26134516e-15\n -8.65619093e-16 -4.68321475e-16 -3.94978317e-17 -8.63439203e-16\n  3.37722937e-16 -1.46516672e-13 -2.71431786e-15  4.82959689e-16\n -8.15101411e-16  1.78565432e-14  1.18347747e-15 -2.35830381e-14]\n"
     ]
    }
   ],
   "source": [
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_img_features)\n",
    "train_img_features = scaler.transform(train_img_features)\n",
    "\n",
    "m1 = np.mean(train_img_features, axis=0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes Linéaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Regression accuracy: 0.2421340629274966 | Training accuracy: 0.24794520547945206\n",
      "LDA accuracy: 0.8207934336525308 | Training accuracy: 0.8210045662100457\n",
      "QDA accuracy: 0.8153214774281806 | Training accuracy: 0.8803652968036529\n",
      "Bayes accuracy: 0.66484268125855 | Training accuracy: 0.6525114155251142\n",
      "KNN accuracy: 0.7920656634746922 | Training accuracy: 0.8662100456621005\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regr = fitLinearRegression(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    for i in range(split_test_y.shape[0]):\n",
    "        f = computeManualFeaturesNormalized(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i], features, colorFeatures, featuresToNormalize, tupleFeatures)[0]\n",
    "\n",
    "        for i in range(len(f[0])): #cleaning None values\n",
    "            if np.isnan(f[0][i]):\n",
    "                f[0][i] = meanOfTrainingFeatures[i]\n",
    "\n",
    "        f = scaler.transform(f) #scale\n",
    "        split_test_y_predicted.append(class_int_round(regr.predict(f), 2))\n",
    "\n",
    "    print(\"Linear Regression accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted)) + \" | Training accuracy: \"+ str(accuracy_score(split_train_y, class_int_round(regr.predict(train_img_features), 2))))\n",
    "else:\n",
    "    print(\"Linear Regression training accuracy: \"+ str(accuracy_score(split_train_y, class_int_round(regr.predict(train_img_features), 2))))\n",
    "\n",
    "\n",
    "# LDA\n",
    "clf_LDA = fitLDA(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    for i in range(split_test_y.shape[0]):\n",
    "        split_test_y_predicted.append(predictForTestSplit(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i],clf_LDA))\n",
    "\n",
    "    print(\"LDA accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf_LDA.predict(train_img_features))))\n",
    "else:\n",
    "    print(\"LDA training accuracy: \"+ str(accuracy_score(split_train_y, clf_LDA.predict(train_img_features))))\n",
    "\n",
    "\n",
    "# QDA\n",
    "clf_QDA = fitQDA(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    for i in range(split_test_y.shape[0]):\n",
    "        split_test_y_predicted.append(predictForTestSplit(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i],clf_QDA))\n",
    "\n",
    "    print(\"QDA accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf_QDA.predict(train_img_features))))\n",
    "else:\n",
    "    print(\"QDA training accuracy: \"+ str(accuracy_score(split_train_y, clf_QDA.predict(train_img_features))))\n",
    "\n",
    "\n",
    "# Bayes\n",
    "clf_Bayes = fitBayes(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    for i in range(split_test_y.shape[0]):\n",
    "        split_test_y_predicted.append(predictForTestSplit(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i],clf_Bayes))\n",
    "\n",
    "    print(\"Bayes accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf_Bayes.predict(train_img_features))))\n",
    "else:\n",
    "    print(\"Bayes training accuracy: \"+ str(accuracy_score(split_train_y, clf_Bayes.predict(train_img_features))))\n",
    "\n",
    "\n",
    "# QDA\n",
    "clf_KNN = fitKNN(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "\n",
    "if boolSplitAndTestLocally:\n",
    "    for i in range(split_test_y.shape[0]):\n",
    "        split_test_y_predicted.append(predictForTestSplit(split_test_img[i], split_test_imgSegCyt[i], split_test_imgSegNuc[i],clf_KNN))\n",
    "\n",
    "    print(\"KNN accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf_KNN.predict(train_img_features))))\n",
    "else:\n",
    "    print(\"KNN training accuracy: \"+ str(accuracy_score(split_train_y, clf_KNN.predict(train_img_features))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (avant SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 28\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: \" + str(numberOfFeatures)+\"\\n\")\n",
    "train_img_features_reduced = train_img_features\n",
    "\n",
    "if boolDimensionReduction:\n",
    "    print(\"Features size before PCA: \"+str(train_img_features.shape))\n",
    "    pca = PCA(n_components=25, random_state=1)\n",
    "    train_img_features_reduced=pca.fit_transform(train_img_features)\n",
    "    print(\"Features size after PCA: \"+str(train_img_features_reduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting Non-linear SVM to the training set\n",
      "{'C': 10.0, 'gamma': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Looking for the best hyperparameters C and Gamma\n",
    "print(\"Fitting Non-linear SVM to the training set\")\n",
    "cList = [1e-3,1e-2,1e-1,1,2,3,4,5,6,7,8,9,1e1]\n",
    "gammaList = [0.0001, 0.0005, 0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "p_grid_nlsvm = {'C': cList,\n",
    "                'gamma': gammaList}\n",
    "NLsvm = SVC(kernel='rbf', probability=True) #use probability=True to make boosting with initialisation bestEstimator\n",
    "grid_nlsvm = GridSearchCV(NLsvm,p_grid_nlsvm,cv=5,scoring=('balanced_accuracy'),return_train_score=True, refit=True, n_jobs=-1) # n_jobs divides computation time by 4 (parallelisation)\n",
    "\n",
    "if boolDimensionReduction:\n",
    "    grid_nlsvm.fit(train_img_features_reduced, split_train_y)\n",
    "else:\n",
    "    grid_nlsvm.fit(train_img_features, split_train_y)\n",
    "\n",
    "print(grid_nlsvm.best_params_)\n",
    "\n",
    "bestEstimator = grid_nlsvm.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NL SVM accuracy: 0.8645690834473324 | Training accuracy: 0.9817351598173516\n"
     ]
    }
   ],
   "source": [
    "split_test_y_predicted = []\n",
    "if boolSplitAndTestLocally:\n",
    "    split_test_y_predicted = predictEntireTestSplit(split_test_img, split_test_imgSegCyt, split_test_imgSegNuc, bestEstimator)\n",
    "\n",
    "    if boolDimensionReduction:\n",
    "        print(\"NL SVM accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, bestEstimator.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"NL SVM accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, bestEstimator.predict(train_img_features))))\n",
    "else:\n",
    "    if boolDimensionReduction:\n",
    "        print(\"NL SVM training accuracy: \"+ str(accuracy_score(split_train_y, bestEstimator.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"NL SVM training accuracy: \"+ str(accuracy_score(split_train_y, bestEstimator.predict(train_img_features))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient boosting accuracy: 0.8344733242134063 | Training accuracy: 0.9662100456621004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "if boolDimensionReduction:\n",
    "    clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=1, random_state=0).fit(train_img_features_reduced, split_train_y)\n",
    "else:\n",
    "    clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=1, random_state=0).fit(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "if boolSplitAndTestLocally:\n",
    "    split_test_y_predicted = predictEntireTestSplit(split_test_img, split_test_imgSegCyt, split_test_imgSegNuc, clf)\n",
    "\n",
    "    if boolDimensionReduction:\n",
    "        print(\"Gradient boosting accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"Gradient boosting accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))\n",
    "else:\n",
    "    if boolDimensionReduction:\n",
    "        print(\"Gradient boosting training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"Gradient boosting training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AdaBoost accuracy: 0.6210670314637483 | Training accuracy: 0.6566210045662101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "if boolDimensionReduction:\n",
    "    clf = AdaBoostClassifier(n_estimators=500, learning_rate=0.1).fit(train_img_features_reduced, split_train_y)\n",
    "else:\n",
    "    clf = AdaBoostClassifier(n_estimators=2000, learning_rate=0.1).fit(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "if boolSplitAndTestLocally:\n",
    "    split_test_y_predicted = predictEntireTestSplit(split_test_img, split_test_imgSegCyt, split_test_imgSegNuc, clf)\n",
    "\n",
    "    if boolDimensionReduction:\n",
    "        print(\"AdaBoost accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"AdaBoost accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))\n",
    "else:\n",
    "    if boolDimensionReduction:\n",
    "        print(\"AdaBoost training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"AdaBoost training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Histogram-Gradient boosting accuracy: 0.8755129958960328 | Training accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "if boolDimensionReduction:\n",
    "    clf = HistGradientBoostingClassifier(max_iter=100, l2_regularization=1, learning_rate=0.1).fit(train_img_features_reduced, split_train_y)\n",
    "else:\n",
    "    clf = HistGradientBoostingClassifier(max_iter=100, l2_regularization=0.75, learning_rate=0.1).fit(train_img_features, split_train_y)\n",
    "\n",
    "split_test_y_predicted = []\n",
    "if boolSplitAndTestLocally:\n",
    "    split_test_y_predicted = predictEntireTestSplit(split_test_img, split_test_imgSegCyt, split_test_imgSegNuc, clf)\n",
    "\n",
    "    if boolDimensionReduction:\n",
    "        print(\"Histogram-Gradient boosting accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"Histogram-Gradient boosting accuracy: \"+ str(accuracy_score(split_test_y, split_test_y_predicted))+ \" | Training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))\n",
    "else:\n",
    "    if boolDimensionReduction:\n",
    "        print(\"Histogram-Gradient boosting training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features_reduced))))\n",
    "    else:\n",
    "        print(\"Histogram-Gradient boosting training accuracy: \"+ str(accuracy_score(split_train_y, clf.predict(train_img_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV pour Méthodes Linéaires, Non linear SVM ou Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not boolSplitAndTestLocally: #predict test set and export CSV\n",
    "    # to modify ___________\n",
    "    fileName = \"NLSVM.csv\"\n",
    "    classifier = bestEstimator\n",
    "    #______________________\n",
    "\n",
    "    submissionCSV(fileName, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}